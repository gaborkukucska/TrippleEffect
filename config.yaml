# START OF FILE config.yaml
# Configuration for TrippleEffect Bootstrap Agents.

# This file defines essential bootstrap agents (like Admin AI).
# The framework will automatically discover reachable providers and available models
# at startup based on your .env configuration and the MODEL_TIER setting.

# --- Automatic Model Selection Notes ---
# - Admin AI: If provider/model below are commented out or invalid, the framework
#   will attempt to select the best available model automatically based on a
#   preferred list (see agent_manager.py) and discovered models. Check startup
#   logs to see which model was selected.
# - Dynamic Agents: Created agents use models validated against the discovered list.

# --- Bootstrap Agent Configurations ---
agents:
  - agent_id: "admin_ai" # The central coordinator agent
    config:
      # --- Provider & Model (Optional Override) ---
      # If you want to FORCE Admin AI to use a specific provider/model,
      # uncomment and set these lines. Ensure the provider is configured
      # in .env and the model is available/discoverable.
      # Otherwise, leave commented out for automatic selection.
      # provider: "openrouter"
      # model: "google/gemini-flash-1.5:free"

      # --- Admin AI Persona & Instructions ---
      # Define the high-level goal/persona here.
      # The detailed operational workflow, tool usage, and the list of *actually available*
      # models will be automatically injected into the prompt by the framework.
      system_prompt: |
        You are the Admin AI, the central coordinator and **primary user interface** for the TrippleEffect multi-agent system.
        Your primary function is to understand user requests, devise a plan involving a team of specialized agents, create those agents *using only the available models listed below*, delegate tasks to them, monitor their progress, synthesize their results, and report the final outcome to the user.
        You are the orchestrator and project manager. You interact directly with the human user for clarification and final reporting.
        Delegate tasks aggressively; do not perform research, writing, coding, etc., yourself.
        Follow the structured workflow and tool usage protocols provided by the framework.
      temperature: 0.6 # Balanced temperature for planning and control
      persona: "Admin AI (@admin_ai)" # Display name
      # Specify provider/model if auto-select isn't desired
      provider: "ollama"
      # Model name MUST include the provider prefix for local models
      model: "ollama/llama3.2:3b-instruct-q4_K_M"

  # --- Add more bootstrap agents here if needed ---
  # Example:
  # - agent_id: "backup_monitor"
  #   config:
  #     provider: "ollama" # Specify provider/model if auto-select isn't desired
  #     model: "llama3.2:3b-instruct-q4_K_M"
  #     system_prompt: "You periodically check system status."
  #     persona: "Backup Monitor (@monitor)"
