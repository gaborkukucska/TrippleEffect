<!-- # START OF FILE README.md -->
# TrippleEffect ğŸ§‘â€ğŸš’ğŸ§‘â€ğŸ«ğŸ‘©â€ğŸ”§

**TrippleEffect** is an asynchronous, collaborative multi-agent framework designed with a browser-based user interface ğŸŒ, optimized for environments like Termux ğŸ“±. It allows multiple Language Model (LLM) agents ğŸ¤–ğŸ¤–ğŸ¤– to work together on complex tasks, coordinated through a central backend and managed via a web UI. It aims for extensibility and supports various LLM API providers, including **OpenAI**, **Ollama**, and **OpenRouter** (and can be extended to others like LiteLLM, Google, Anthropic, etc.).

## ğŸ¯ Core Concept

The system orchestrates multiple LLM agents, whose number and specific configurations (**provider**, model, persona, system prompt, etc.) are defined in a central `config.yaml` file âš™ï¸. Users interact with the system through a web interface to submit tasks via text ğŸ“ (voice ğŸ¤, camera ğŸ“¸, file uploads ğŸ“ are planned future features).

The agents, loaded based on the configuration:
*   Interact with their configured LLM provider (**OpenAI**, **Ollama**, **OpenRouter**, etc.).
*   Can work concurrently on the same task.
*   Collaborate and delegate sub-tasks (future phase ğŸ¤).
*   Utilize tools within sandboxed environments ğŸ› ï¸ (supported across providers where models allow).
*   Stream their responses back to the user interface in real-time âš¡.

## âœ¨ Key Features

*   **Multi-Agent Architecture:** Supports multiple LLM agents working concurrently.
*   **Asynchronous Backend:** Built with FastAPI and `asyncio` for efficient handling of concurrent operations.
*   **Browser-Based UI:** Simple web interface for task submission, agent monitoring, viewing configurations, and results.
*   **Real-time Updates:** Uses WebSockets (`/ws`) for instant communication.
*   **Multi-Provider LLM Support:** Connect agents to different LLM backends (**OpenAI**, local **Ollama**, **OpenRouter**, easily extensible).
*   **YAML Configuration:** Easily define agents (ID, **provider**, model, system prompt, persona, temperature, provider-specific args) via `config.yaml`. Defaults and API keys set via `.env`.
*   **Sandboxed Workspaces:** Each agent operates within its own directory (`sandboxes/agent_<id>/`) for file-based tasks ğŸ“.
*   **Tool Usage:** Framework allows agents to use tools (e.g., file system access ğŸ“„, web search ğŸ” planned). Tool support relies on the capabilities of the chosen LLM model and provider implementation.
*   **Extensible Design:** Modular structure (`src/llm_providers`, `src/tools`) for adding new LLM providers, agents, or tools.
*   **Termux Friendly:** Aims for compatibility and reasonable performance on resource-constrained environments. Requires specific build tools.

## ğŸ—ï¸ Architecture Overview (Conceptual - Updated for Provider Abstraction)

```mermaid
graph LR
    subgraph Frontend
        UI["ğŸŒ Browser UI <br>(HTML/CSS/JS)<br>Includes Config View"]
    end

    subgraph Backend
        FASTAPI["ğŸš€ FastAPI Backend <br>(main.py, api/)<br>+ Config Read Route"]
        WS_MANAGER["ğŸ”Œ WebSocket Manager <br>(api/websocket_manager.py)"]
        AGENT_MANAGER["ğŸ§‘â€ğŸ’¼ Agent Manager <br>(agents/manager.py)"]
        subgraph Agents
            direction LR
            AGENT_INST_1["ğŸ¤– Agent Instance 1 <br>(agents/core.py)<br>Uses Provider A"]
            AGENT_INST_2["ğŸ¤– Agent Instance 2 <br>Uses Provider B"]
            AGENT_INST_N["ğŸ¤– Agent Instance N <br>Uses Provider C"]
        end
        subgraph LLM_Providers ["â˜ï¸ LLM Providers <br>(src/llm_providers/)"]
            PROVIDER_A["ğŸ”Œ Provider A <br>(e.g., OpenAI)"]
            PROVIDER_B["ğŸ”Œ Provider B <br>(e.g., Ollama)"]
            PROVIDER_C["ğŸ”Œ Provider C <br>(e.g., OpenRouter)"]
        end
        subgraph Tools
            direction TB
            TOOL_EXECUTOR["ğŸ› ï¸ Tool Executor <br>(tools/executor.py)"]
            TOOL_FS["ğŸ“„ FileSystem Tool <br>(tools/file_system.py)"]
            TOOL_WEB["ğŸ” Web Search Tool (Planned)"]
        end
        SANDBOXES["ğŸ“ Sandboxes <br>(sandboxes/agent_id/)"]
    end

    subgraph External
        LLM_API_SVC["â˜ï¸ External LLM APIs <br>(OpenAI, OpenRouter)"]
        OLLAMA_SVC["âš™ï¸ Local Ollama Service"]
        CONFIG_YAML["âš™ï¸ config.yaml <br>(Read Only by App)"]
        DOT_ENV[".env File <br>(API Keys, URLs - Read Only by App)"]
    end

    %% --- Connections ---
    UI -- HTTP --> FASTAPI;
    UI -- "WebSocket /ws" <--> WS_MANAGER;
    FASTAPI -- Manages --> AGENT_MANAGER;
    FASTAPI -- "Config Read API /api/config/agents" --> CONFIG_YAML; # Reads config via settings
    WS_MANAGER -- "Forwards/Receives" --> AGENT_MANAGER;
    AGENT_MANAGER -- "Controls/Coordinates" --> AGENT_INST_1;
    AGENT_MANAGER -- "Controls/Coordinates" --> AGENT_INST_2;
    AGENT_MANAGER -- "Controls/Coordinates" --> AGENT_INST_N;
    AGENT_MANAGER -- "Reads Config" --> CONFIG_YAML; # Via settings
    AGENT_MANAGER -- "Reads Defaults/Secrets" --> DOT_ENV; # Via settings
    AGENT_MANAGER -- "Instantiates & Injects" --> LLM_Providers;
    AGENT_INST_1 -- Uses --> PROVIDER_A;
    AGENT_INST_2 -- Uses --> PROVIDER_B;
    AGENT_INST_N -- Uses --> PROVIDER_C;
    PROVIDER_A -- Interacts --> LLM_API_SVC;
    PROVIDER_B -- Interacts --> OLLAMA_SVC;
    PROVIDER_C -- Interacts --> LLM_API_SVC;
    AGENT_MANAGER -- "Routes Tool Request" --> TOOL_EXECUTOR;
    TOOL_EXECUTOR -- Executes --> TOOL_FS;
    TOOL_EXECUTOR -- Executes --> TOOL_WEB;
    %% Tool requests flow through provider
    AGENT_INST_1 -- "Requests Tools Via Provider" --> LLM_Providers;
    AGENT_INST_1 -- "File I/O Via Tool" --> SANDBOXES;
    TOOL_FS -- "Operates Within" --> SANDBOXES;

```

*   **LLM Providers (`src/llm_providers/`):** New layer abstracting interaction with different LLM APIs (OpenAI, Ollama, OpenRouter).
*   **Agent Instances:** Use an injected LLM provider instance.
*   **`.env`:** Crucial for storing API keys and provider base URLs.

## ğŸ’» Technology Stack

*   **Backend:** Python 3.9+, FastAPI, Uvicorn
*   **WebSockets:** `websockets` library integrated with FastAPI
*   **LLM Interaction:** `openai` library (used by OpenAI & OpenRouter providers), `aiohttp` (used by Ollama provider)
*   **Frontend:** HTML5, CSS3, Vanilla JavaScript
*   **Asynchronous:** `asyncio`
*   **Configuration:** YAML (`PyYAML`), `.env` files (`python-dotenv`)
*   **Data Handling:** Pydantic (via FastAPI)

## ğŸ“ Directory Structure

```TrippleEffect/
â”œâ”€â”€ .venv/
â”œâ”€â”€ config.yaml             # Agent configurations (provider, model, etc.) âœ¨ UPDATED
â”œâ”€â”€ helperfiles/            # Project planning & tracking ğŸ“
â”‚   â”œâ”€â”€ PROJECT_PLAN.md
â”‚   â”œâ”€â”€ DEVELOPMENT_RULES.md
â”‚   â””â”€â”€ FUNCTIONS_INDEX.md
â”œâ”€â”€ sandboxes/              # Agent work directories (created at runtime) ğŸ“
â”‚   â””â”€â”€ agent_X/
â”œâ”€â”€ src/                    # Source code ğŸ
â”‚   â”œâ”€â”€ agents/             # Agent core logic & manager
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ core.py         # Agent class (uses injected provider) ğŸ¤–
â”‚   â”‚   â””â”€â”€ manager.py      # AgentManager (instantiates providers) ğŸ§‘â€ğŸ’¼
â”‚   â”œâ”€â”€ api/                # FastAPI routes & WebSocket logic ğŸ”Œ
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ http_routes.py
â”‚   â”‚   â””â”€â”€ websocket_manager.py
â”‚   â”œâ”€â”€ config/             # Configuration loading âš™ï¸
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â””â”€â”€ settings.py     # Loads .env and config.yaml (provider keys/URLs) âœ¨ UPDATED
â”‚   â”œâ”€â”€ llm_providers/      # LLM provider implementations <--- âœ¨ NEW
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ base.py         # BaseLLMProvider ABC
â”‚   â”‚   â”œâ”€â”€ ollama_provider.py
â”‚   â”‚   â”œâ”€â”€ openai_provider.py
â”‚   â”‚   â””â”€â”€ openrouter_provider.py
â”‚   â”œâ”€â”€ tools/              # Agent tools implementations ğŸ› ï¸
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ base.py
â”‚   â”‚   â”œâ”€â”€ executor.py
â”‚   â”‚   â””â”€â”€ file_system.py
â”‚   â”œâ”€â”€ ui/                 # UI backend helpers (if needed)
â”‚   â”‚   â””â”€â”€ __init__.py
â”‚   â”œâ”€â”€ utils/              # Utility functions
â”‚   â”‚   â””â”€â”€ __init__.py
â”‚   â”œâ”€â”€ __init__.py
â”‚   â””â”€â”€ main.py             # Application entry point ğŸš€
â”œâ”€â”€ static/                 # Frontend static files ğŸŒ
â”‚   â”œâ”€â”€ css/
â”‚   â”‚   â””â”€â”€ style.css
â”‚   â””â”€â”€ js/
â”‚       â””â”€â”€ app.js          # âœ¨ UPDATED for Config View
â”œâ”€â”€ templates/              # HTML templates (Jinja2)
â”‚   â””â”€â”€ index.html
â”œâ”€â”€ .env.example            # Example environment variables âœ¨ UPDATED
â”œâ”€â”€ .gitignore
â”œâ”€â”€ LICENSE                 # Project License (Specify one!) ğŸ“œ
â”œâ”€â”€ README.md               # This file! ğŸ“–
â””â”€â”€ requirements.txt        # Python dependencies ğŸ“¦ âœ¨ UPDATED (uvicorn)
```

## âš™ï¸ Installation

1.  **Prerequisites:**
    *   Python 3.9+ ğŸ
    *   Git
    *   (Optional) Local Ollama instance running if using Ollama provider.
    *   **Termux specific:** Some Python packages require compilation. Install necessary build tools using `pkg`:
        ```bash
        pkg update && pkg upgrade
        pkg install binutils build-essential -y
        ```
        *(Note: `python-dev` seems included in the main `python` package now).*

2.  **Clone Repository:**
    ```bash
    git clone https://github.com/gaborkukucska/TrippleEffect.git
    cd TrippleEffect
    ```

3.  **Set up Virtual Environment:** (Recommended)
    ```bash
    python -m venv .venv
    source .venv/bin/activate # Linux/macOS/Termux
    # .venv\Scripts\activate # Windows
    ```

4.  **Install Dependencies:** ğŸ“¦
    ```bash
    # Optional: Upgrade pip
    pip install --upgrade pip
    # Install project requirements
    pip install -r requirements.txt
    ```

5.  **Configure Environment Variables:** ğŸ”‘
    *   Copy `.env.example` to `.env`:
        ```bash
        cp .env.example .env
        ```
    *   **Edit the `.env` file:** Add your API keys and configure URLs as needed for the providers you intend to use (OpenAI, OpenRouter, Ollama). Pay attention to `OPENROUTER_REFERER`.
        ```dotenv
        # .env (Example - Fill with your actual values)
        OPENAI_API_KEY=sk-your-openai-key...
        # OPENAI_BASE_URL=

        OPENROUTER_API_KEY=sk-or-v1-your-openrouter-key...
        # OPENROUTER_BASE_URL=
        OPENROUTER_REFERER=http://localhost:8000/ # Or your app name

        OLLAMA_BASE_URL=http://localhost:11434 # Adjust if Ollama runs elsewhere

        # Default agent params (if needed)
        DEFAULT_AGENT_PROVIDER="openai"
        DEFAULT_AGENT_MODEL="gpt-3.5-turbo"
        # ... other defaults
        ```
    *   **Important:** Ensure `.env` is listed in your `.gitignore` file!

6.  **Configure Agents:** ğŸ§‘â€ğŸ”§ğŸ§‘â€ğŸ«ğŸ§‘â€ğŸš’
    *   Edit the `config.yaml` file in the project root.
    *   For each agent, define:
        *   `agent_id`: Unique identifier.
        *   `provider`: `"openai"`, `"ollama"`, or `"openrouter"`.
        *   `model`: The model name specific to the chosen provider (e.g., `"gpt-4-turbo"`, `"llama3"`, `"mistralai/mistral-7b-instruct"`).
        *   `system_prompt`, `temperature`, `persona`.
        *   Optionally add provider-specific parameters (like `base_url`, `referer`) to override `.env` defaults for specific agents.

## â–¶ï¸ Running the Application

```bash
python src/main.py
```

*   The server will start (usually on port 8000).
*   It loads agents based on `config.yaml` and initializes their respective LLM providers. Check console output for details and potential configuration warnings.
*   Access the UI in your web browser: `http://localhost:8000`.

## ğŸ–±ï¸ Usage

1.  Open the web UI.
2.  The backend loads agents. You should see a "Connected" status and agent configurations/statuses loaded in their respective UI sections.
3.  Type your task into the input box âŒ¨ï¸ (optionally attach a text file using the ğŸ“ button).
4.  Send the message. The task goes concurrently to all initialized and *available* (idle) agents.
5.  Observe responses streaming back in the "Conversation Area", identified by `agent_id`. System messages and errors appear in the "System Logs & Status" area.
6.  Agent behavior (including tool use) depends on the configured provider and model. Check agent status updates in the "Agent Status" section.
7.  Agents operate within `sandboxes/agent_<id>/` for file system tool operations.

## ğŸ› ï¸ Development

*   **Code Style:** Follow PEP 8. Use formatters like Black.
*   **Linting:** Use Flake8 or Pylint.
*   **Helper Files:** Keep `helperfiles/PROJECT_PLAN.md` and `helperfiles/FUNCTIONS_INDEX.md` updated! âœï¸
*   **Configuration:** Modify `config.yaml` to add/change agents/providers. Set API keys/URLs/defaults in `.env`. (UI config editing planned for Phase 8).
*   **Branching:** Use feature branches.

## ğŸ™Œ Contributing

Contributions welcome! Follow guidelines, open Pull Requests.

## ğŸ“œ License

(Specify MIT License, Apache 2.0, etc.)
