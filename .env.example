# START OF FILE .env.example

# Copy this file to .env and fill in your details.
# Remove or comment out lines for providers you are not using.

# --- LLM Provider API Keys & Configuration ---

# -- OpenRouter (REQUIRED if used) --
# Supports multiple keys: OPENROUTER_API_KEY, OPENROUTER_API_KEY_1, OPENROUTER_API_KEY_2, ...
OPENROUTER_API_KEY="sk-or-v1-..."
# OPENROUTER_API_KEY_1="sk-or-v1-..."
# OPENROUTER_API_KEY_2="sk-or-v1-..."
OPENROUTER_BASE_URL="https://openrouter.ai/api/v1" # Optional: Default is provided
OPENROUTER_REFERER="http://localhost:8000/YourAppName" # Optional: Set your app name or URL

# -- OpenAI (Optional) --
# Supports multiple keys: OPENAI_API_KEY, OPENAI_API_KEY_1, ...
# OPENAI_API_KEY="sk-..."
# #OPENAI_API_KEY_1="sk-..."
# OPENAI_BASE_URL="" # Optional: Use if you have a proxy/custom endpoint

# -- LiteLLM (Optional - for local proxy/models) --
# Supports multiple keys if your LiteLLM config requires it: LITELLM_API_KEY, LITELLM_API_KEY_1, ...
LITELLM_API_KEY="" # Only needed if your LiteLLM instance requires a master key
# Base URL is handled by automatic discovery.

# -- Ollama (Optional - for local models) --
# No API key needed. Base URL is handled by automatic discovery.

# --- Ollama Proxy Settings (REMOVED) ---

# -- Other Providers (Optional - Add if needed) --
# ANTHROPIC_API_KEY="sk-ant-..."
# ANTHROPIC_API_KEY_1="sk-ant-..."
# GOOGLE_API_KEY="..."
# DEEPSEEK_API_KEY="..."

# --- Model Filtering Tier ---
# Controls which models from *remote* providers (OpenRouter, OpenAI) are made available.
# Local providers (Ollama, LiteLLM) are always included if reachable.
# Valid options:
#   ALL  = Include all discovered models (free and paid).
#   FREE = Include only models identified as free (currently checks for ':free' in OpenRouter IDs).
MODEL_TIER="FREE"

# --- Default Agent Settings ---
# Used if not specified in config.yaml for bootstrap agents or during dynamic creation
# DEFAULT_AGENT_PROVIDER="openrouter"
# DEFAULT_AGENT_MODEL="google/gemini-flash-1.5:free"
# DEFAULT_SYSTEM_PROMPT="You are a helpful assistant."
# DEFAULT_TEMPERATURE="0.7"
# DEFAULT_PERSONA="Assistant Agent"

# --- Tool Configuration ---
# GitHub Tool (Optional) - Requires 'repo' scope for full functionality
# GITHUB_ACCESS_TOKEN=""

# --- Tavily Search Key ---
# Add your Tavily API Key here.
# TAVILY_API_KEY=

# --- Project/Session Configuration ---
# Optional: Define where project session data is stored. Defaults to 'projects/' in the root.
# PROJECTS_BASE_DIR="./my_projects"

# --- Local API Discovery ---
# Enable/disable scanning for local APIs (Ollama, LiteLLM, etc.)
LOCAL_API_SCAN_ENABLED=true
# Ports to check during the scan
LOCAL_API_SCAN_PORTS="11434,8000"
# Timeout (in seconds) for attempting connection to each IP/port during scan
LOCAL_API_SCAN_TIMEOUT=0.5
# LOCAL_API_SCAN_SUBNET removed - Network range is determined automatically using netifaces/nmap

# Retry and Failover Settings
MAX_STREAM_RETRIES=3
RETRY_DELAY_SECONDS=5.0
MAX_FAILOVER_ATTEMPTS=3

# --- Logging Configuration (Optional) ---
# Valid options: DEBUG, INFO, WARNING, ERROR, CRITICAL
# LOG_LEVEL=INFO
