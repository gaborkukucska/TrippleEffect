# START OF FILE .env.example

# Copy this file to .env and fill in your details.
# Remove or comment out lines for providers you are not using.

# --- LLM Provider API Keys & Configuration ---

# -- OpenRouter (REQUIRED if used) --
# Supports multiple keys: OPENROUTER_API_KEY, OPENROUTER_API_KEY_1, OPENROUTER_API_KEY_2, ...
OPENROUTER_API_KEY="sk-or-v1-..."
# OPENROUTER_API_KEY_1="sk-or-v1-..."
# OPENROUTER_API_KEY_2="sk-or-v1-..."
OPENROUTER_BASE_URL="https://openrouter.ai/api/v1" # Optional: Default is provided
OPENROUTER_REFERER="http://localhost:8000/YourAppName" # Optional: Set your app name or URL

# -- OpenAI (Optional) --
# Supports multiple keys: OPENAI_API_KEY, OPENAI_API_KEY_1, ...
# OPENAI_API_KEY="sk-..."
# #OPENAI_API_KEY_1="sk-..."
# OPENAI_BASE_URL="" # Optional: Use if you have a proxy/custom endpoint

# -- LiteLLM (Optional - for local proxy) --
# Supports multiple keys if your LiteLLM config requires it: LITELLM_API_KEY, LITELLM_API_KEY_1, ...
LITELLM_API_KEY="" # Only needed if your LiteLLM instance requires a master key
LITELLM_BASE_URL="http://localhost:4000" # Optional: Default localhost assumed if not set

# -- Ollama (Optional - for local models) --
# No API key needed by default. Only Base URL is relevant.
OLLAMA_BASE_URL="http://localhost:11434" # Optional: Default localhost assumed if not set. This is overridden if USE_OLLAMA_PROXY=true.

# --- Ollama Proxy Settings (Optional) ---
# Set USE_OLLAMA_PROXY to true to automatically start and use the integrated Node.js proxy
# This can help resolve connection issues with Ollama, especially during streaming.
USE_OLLAMA_PROXY=false
# Port the integrated proxy server will listen on
OLLAMA_PROXY_PORT=3000
# The actual Ollama API endpoint the proxy should forward requests to (used by the proxy itself)
OLLAMA_PROXY_TARGET_URL="http://localhost:11434"

# -- Other Providers (Optional - Add if needed) --
# ANTHROPIC_API_KEY="sk-ant-..."
# ANTHROPIC_API_KEY_1="sk-ant-..."
# GOOGLE_API_KEY="..."
# DEEPSEEK_API_KEY="..."

# --- Model Filtering Tier ---
# Controls which models from *remote* providers (OpenRouter, OpenAI) are made available.
# Local providers (Ollama, LiteLLM) are always included if reachable.
# Valid options:
#   ALL  = Include all discovered models (free and paid).
#   FREE = Include only models identified as free (currently checks for ':free' in OpenRouter IDs).
MODEL_TIER="FREE"

# --- Default Agent Settings ---
# Used if not specified in config.yaml for bootstrap agents or during dynamic creation
# DEFAULT_AGENT_PROVIDER="openrouter"
# DEFAULT_AGENT_MODEL="google/gemini-flash-1.5:free"
# DEFAULT_SYSTEM_PROMPT="You are a helpful assistant."
# DEFAULT_TEMPERATURE="0.7"
# DEFAULT_PERSONA="Assistant Agent"

# --- Tool Configuration ---
# GitHub Tool (Optional) - Requires 'repo' scope for full functionality
# GITHUB_ACCESS_TOKEN=""

# --- Tavily Search Key ---
# Add your Tavily API Key here.
# TAVILY_API_KEY=

# --- Project/Session Configuration ---
# Optional: Define where project session data is stored. Defaults to 'projects/' in the root.
# PROJECTS_BASE_DIR="./my_projects"

# --- Logging Configuration (Optional) ---
# LOG_LEVEL="INFO" # DEBUG, INFO, WARNING, ERROR, CRITICAL
