# START OF FILE .env.example

# --- LLM Provider Configurations ---

# OpenAI (Optional: Fill in if you want to use OpenAI models)
OPENAI_API_KEY=""
# OPENAI_BASE_URL="" # Optional: Use if you have a custom OpenAI-compatible endpoint

# OpenRouter (Optional: Fill in if you want to use OpenRouter models)
OPENROUTER_API_KEY=""
OPENROUTER_BASE_URL="https://openrouter.ai/api/v1" # Default, can be overridden
# Add your website/app URL - Recommended by OpenRouter for tracking/moderation
OPENROUTER_REFERER="http://localhost:8000/TrippleEffect" # Example - CHANGE THIS

# Ollama (Required if using Ollama provider)
OLLAMA_BASE_URL="http://localhost:11434" # Default local Ollama URL

# --- Agent Defaults ---
# Default provider and model if not specified in config.yaml
DEFAULT_AGENT_PROVIDER="openrouter"
DEFAULT_AGENT_MODEL="google/gemini-flash-1.5:free" # Example free model
DEFAULT_SYSTEM_PROMPT="You are a helpful assistant."
DEFAULT_TEMPERATURE="0.7"
DEFAULT_PERSONA="General Assistant"

# --- Project/Session Configuration ---
# Base directory for storing project/session data (defaults to 'projects/' in root)
# PROJECTS_BASE_DIR="path/to/your/projects"

# --- Tool Configurations ---
# GitHub Tool (Optional: Required only if using GitHubTool)
# Create a Personal Access Token (PAT) with 'repo' scope: https://github.com/settings/tokens
GITHUB_ACCESS_TOKEN=""

# --- Logging Configuration (Example - Not currently used by code) ---
# LOG_LEVEL="INFO" # e.g., DEBUG, INFO, WARNING, ERROR, CRITICAL
# LOG_FILE_PATH="logs/app.log" # Path relative to project root
